{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pointnet4BerriesHierarchicalSegmentation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nSOBP9AGPDMp","colab_type":"text"},"source":["## Clone Github Repository"]},{"cell_type":"code","metadata":{"id":"u1AD7FpqMAzG","colab_type":"code","outputId":"015951e1-30ce-48a1-ab3b-92b7589146ba","executionInfo":{"status":"ok","timestamp":1585327116388,"user_tz":-60,"elapsed":5111,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# clone repository\n","!git clone https://github.com/ndoll1998/Pointnet4Berries.git P4B"],"execution_count":1,"outputs":[{"output_type":"stream","text":["fatal: destination path 'P4B' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OwkVe7kaPHxj","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"NwtjKiLgPS_o","colab_type":"code","outputId":"840d03ad-c813-4fa5-a824-837eeb3148d2","executionInfo":{"status":"ok","timestamp":1585327116823,"user_tz":-60,"elapsed":5526,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# imports\n","import sys\n","import numpy as np\n","import torch\n","from sklearn.neighbors import NearestNeighbors\n","from torch.utils.data import TensorDataset, DataLoader\n","# import model and utils\n","from P4B.Pointnet.models import Model_SEG\n","from P4B.utils.data import build_data_seg, class2color, seg_file_features\n","from P4B.utils.utils import compute_fscores, normalize_pc, align_principle_component, interpolate_pc\n","from P4B.utils.augmentation import Augmenter, augment_rotate_pointcloud\n","from P4B.utils.clustering import region_growing\n","from P4B.utils.torchBoard import TorchBoard, ConfusionMatrix\n","# import others\n","import os\n","import json\n","from time import time\n","from tqdm import tqdm\n","from math import ceil\n","from random import sample\n","from collections import OrderedDict\n","# import google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cEQxdgCtPln2","colab_type":"text"},"source":["## Set Up"]},{"cell_type":"code","metadata":{"id":"YOH9mghbPmmb","colab_type":"code","colab":{}},"source":["# cude device to use\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","# hierarchical mapping\n","hierarchical_class_bins = [\n","    # First hierarchy\n","    OrderedDict({\n","        'twig:temp': ['twig', 'subtwig', 'berry'],\n","        'rachis:final': ['rachis', 'peduncle']\n","    }),\n","    # Second hierarchy - all classes in last hierarchy must be final\n","    OrderedDict({\n","        'subtwig:final': ['subtwig', 'berry'],\n","        'twig:final': ['twig']\n","    })\n","]; K = len(hierarchical_class_bins[0])\n","# augmentations\n","augmentations = [\n","    Augmenter(augment_rotate_pointcloud, feats=seg_file_features, apply_count=20, rot_axis='xyz')\n","]\n","# used features\n","features = ['x', 'y', 'z', 'r', 'g', 'b'] #, 'length-xy', 'curvature']\n","feature_dim = len(features) - 3\n","# data preprocessing\n","align_pointclouds = False\n","interpolate_pointclouds = True\n","# number of points and samples\n","n_points = 70_000\n","n_samples = 5\n","# number of poinclouds per class for testing\n","n_test_pcs = 1\n","# initial checkpoint\n","encoder_init_checkpoint = None\n","segmentater_init_checkpoint = None\n","# training parameters\n","epochs = 500\n","batch_size = 4\n","# optimizer parameters\n","lr = 5e-4\n","weight_decay = 1e-3\n","# path to files\n","fpath = \"drive/My Drive/P4B/data/Segmentation\"\n","# save path\n","save_path = \"drive/My Drive/P4B/results/hierarchicalSegmentation_basic_v3\"\n","os.makedirs(save_path, exist_ok=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lZfEKZ5QFjO","colab_type":"text"},"source":["## Load and prepare Data"]},{"cell_type":"code","metadata":{"id":"HjlacUiJQHB1","colab_type":"code","outputId":"47b1b61b-4bdc-4f81-ef09-eadb33bf7604","executionInfo":{"status":"ok","timestamp":1585327150115,"user_tz":-60,"elapsed":38785,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pointclouds = {}\n","# open files\n","for fname in tqdm(os.listdir(fpath)):\n","    # get name of pointcloud\n","    class_name, name = fname.replace('.xyzrgbc', '').split('_')[:2]\n","    # check for entry in pointclouds\n","    if class_name not in pointclouds:\n","        pointclouds[class_name] = {}\n","    if name not in pointclouds[class_name]:\n","        pointclouds[class_name][name] = []\n","    # create full path to file\n","    full_path = os.path.join(fpath, fname)\n","    # read pointcloud\n","    pointclouds[class_name][name].append(np.loadtxt(full_path, dtype=np.float32))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["100%|██████████| 14/14 [00:33<00:00,  2.37s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0GCDZDXOQ4XR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ac12c7bf-d731-4dd4-8f0d-c73d37207427","executionInfo":{"status":"ok","timestamp":1585327150117,"user_tz":-60,"elapsed":38770,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}}},"source":["# separate pointclouds into training and testing samples\n","train_pointclouds, test_pointclouds = {}, {}\n","for class_name, pcs in pointclouds.items():\n","    # get random subset to train from\n","    train_pc_names = [list(pcs.keys())[i] for i in range(n_test_pcs, len(pcs))] # sample(pcs.keys(), len(pcs) - n_test_pcs)\n","    test_pc_names = [list(pcs.keys())[i] for i in range(n_test_pcs)]                         # set(pcs.keys()) - set(train_pc_names)\n","    print(train_pc_names, test_pc_names)\n","    # add to dicts\n","    train_pointclouds[class_name] = sum([pcs[n] for n in train_pc_names], [])\n","    test_pointclouds[class_name] = sum([pcs[n] for n in test_pc_names], [])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['2E.feats', '3E.feats', '4E.feats'] ['1E.feats']\n","['2D.feats', '3D.feats', '4D.feats', '5D.feats'] ['1D.feats']\n","['2.feats', '4.feats', '5.feats', '3.feats'] ['1.feats']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sw6lM557Q70q","colab_type":"text"},"source":["## Build Training and Testing Data"]},{"cell_type":"code","metadata":{"id":"HksGra_zRBg8","colab_type":"code","outputId":"afcc53ca-4b12-44f7-9afa-7aed2dea3383","executionInfo":{"status":"ok","timestamp":1585327913904,"user_tz":-60,"elapsed":802537,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["def preprocess(pc):\n","    if align_pointclouds:\n","        # align pointcloud\n","        pc[:, :3] = align_principle_component(pc[:, :3])\n","\n","    if (pc.shape[0] < n_points) and interpolate_pointclouds:\n","        # interpolate pointcloud\n","        k = ceil(n_points/pc.shape[0] - 1)\n","        points = interpolate_pc(pc[:, :3], pc[:, 6:9], k=k)\n","        # get nearest neighbors of each point\n","        tree = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(pc[:, :3])\n","        neighbor_idx = tree.kneighbors(points, return_distance=False)\n","        # interpolate features from nearest neighbors\n","        feats = pc[neighbor_idx, 3:-1].mean(axis=1)\n","        # get label from source point of each interpolated point\n","        labels = pc[:, -1:].repeat(k, axis=0)\n","        # concatenate all\n","        ipc = np.concatenate((points, feats, labels), axis=1)\n","        pc = np.concatenate((pc, ipc), axis=0)\n","\n","    # return pointcloud\n","    return pc\n","\n","def build_data(pointclouds):\n","    # remove modifiers from keys, i.e. :final, etc.\n","    make_class_bins = lambda class_bins: OrderedDict({key.split(':')[0]: value for key, value in class_bins.items()})\n","\n","    data = []\n","    # build first hierarchy data\n","    class_bins = hierarchical_class_bins[0]\n","    data.append(build_data_seg(pointclouds, n_points, n_samples, make_class_bins(class_bins), features=features, augmentations=augmentations))\n","\n","    # build data for following hierarchies\n","    for class_bins in hierarchical_class_bins[1:]:\n","        class_bin_ids = [list(class2color.keys()).index(n) for bin in class_bins.values() for n in bin]\n","        # build pointclouds of next hierarchy\n","        next_pointclouds = {}\n","        # go through all pointclouds\n","        for class_name, pcs in tqdm(pointclouds.items()):\n","            # print(class_name)\n","            next_pointclouds[class_name] = []\n","            for i, pc in enumerate(pcs):\n","                # remove points of classes not contained in any bin\n","                pc = pc[np.isin(pc[:, -1], class_bin_ids), :]\n","                points = normalize_pc(pc[:, :3])\n","                # apply region growing\n","                curvature, normals = pc[:, 9], pc[:, 6:9]\n","                cluster_mask = region_growing(points, normals, curvature, min_points=1_500)\n","                # get cluster ids ignoring outliers\n","                cluster_idx = np.unique(cluster_mask)\n","                cluster_idx = cluster_idx[(cluster_idx != -1)]\n","                # create unnormalized but aligned pointclouds from clusters\n","                next_pointclouds[class_name].extend([preprocess(pc[cluster_mask==i]) for i in cluster_idx])\n","        # update pointclouds\n","        pointclouds = next_pointclouds\n","        # build data from pointclouds - dont apply augmentations to subpointclouds\n","        data.append(build_data_seg(pointclouds, n_points, n_samples//3, make_class_bins(class_bins), features=features))\n","\n","    # concatenate data\n","    return tuple(torch.cat(values, dim=0) for values in zip(*data))\n","\n","# create testing and training data\n","train_data = TensorDataset(*build_data(train_pointclouds))\n","test_data = TensorDataset(*build_data(test_pointclouds))\n","# create dataloaders\n","train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, shuffle=False, batch_size=1)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["pc: 100%|██████████| 231/231 [05:58<00:00,  1.55s/it]\n","100%|██████████| 3/3 [02:30<00:00, 50.18s/it]\n","pc: 100%|██████████| 152/152 [00:52<00:00,  2.89it/s]\n","pc: 100%|██████████| 63/63 [01:53<00:00,  1.81s/it]\n","100%|██████████| 3/3 [00:51<00:00, 17.08s/it]\n","pc: 100%|██████████| 36/36 [00:12<00:00,  2.85it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xQFFYYz_QcpQ","colab_type":"text"},"source":["## Create Model and Optimizer"]},{"cell_type":"code","metadata":{"id":"5KmuGuEzQgiS","colab_type":"code","colab":{}},"source":["# create model\n","model = Model_SEG(K=K, feat_dim=feature_dim)\n","model.load_encoder(encoder_init_checkpoint)\n","model.load_segmentater(segmentater_init_checkpoint)\n","model.to(device)\n","# create optimizer\n","optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1MCpvvCbiwjM","colab_type":"text"},"source":["## Save Configuration"]},{"cell_type":"code","metadata":{"id":"YY7-L6CRiyTx","colab_type":"code","colab":{}},"source":["# build config\n","config = {\n","    \"task\": \"hierarchical_segmentation\",\n","    \"augmentation\": [augment.dict() for augment in augmentations],\n","    \"preparation\": {\n","        \"align_pointclouds\": align_pointclouds,\n","        \"interpolate_pointclouds\": interpolate_pointclouds\n","    },\n","    \"data\": {\n","        \"hierarchy_classes\": hierarchical_class_bins,\n","        \"features\": features,\n","        \"feature_dim\": feature_dim,\n","        \"n_points\": n_points,\n","        \"n_samples\": n_samples, \n","        \"n_test_pointclouds\": n_test_pcs,\n","        \"n_train_samples\": len(train_data),\n","        \"n_train_points\": dict(zip(hierarchical_class_bins[0].keys(), map(int, np.bincount(train_data[:][-1].flatten().numpy())))),\n","        \"n_test_samples\": len(test_data),\n","        \"n_test_points\": dict(zip(hierarchical_class_bins[0].keys(), map(int, np.bincount(test_data[:][-1].flatten().numpy())))),\n","    },\n","    \"training\": {\n","        \"epochs\": epochs,\n","        \"batch_size\": batch_size,\n","    },\n","    \"optimizer\": {\n","        \"learning_rate\": lr,\n","        \"weight_decay\": weight_decay\n","    }\n","}\n","# save to file\n","with open(os.path.join(save_path, \"config.json\"), 'w+') as f:\n","    json.dump(config, f, indent=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNsKVXabjgf5","colab_type":"text"},"source":["## Train and evaluate Model"]},{"cell_type":"code","metadata":{"id":"ia8YZYDCjjKv","colab_type":"code","outputId":"6654dabd-962b-4f63-b38b-cc224baa1485","executionInfo":{"status":"ok","timestamp":1583101265587,"user_tz":-60,"elapsed":3780241,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":459}},"source":["# track losses and f-scores\n","tb = TorchBoard(\"Train_Loss\", \"Test_Loss\", *hierarchical_class_bins[0].keys())\n","tb.add_stat(ConfusionMatrix(hierarchical_class_bins[0].keys(), name=\"Confusion\", normalize=True))\n","# compute loss weights of each class by the number of points associated\n","weight = torch.from_numpy(1 / np.power(np.bincount(train_data[:][-1].flatten().numpy()), 1))\n","weight = (weight / weight.sum()).float().to(device)\n","\n","best_fscore, start = -1, time()\n","for epoch in range(epochs):\n","\n","    # train model\n","    model.train()\n","    # reset for epoch\n","    start_epoch = time()\n","    running_loss = 0\n","\n","    # train loop\n","    for i, (x, y_hat) in enumerate(train_dataloader):\n","        optim.zero_grad()\n","\n","        # pass through model\n","        y = model.forward(x.to(device))\n","        # compute error\n","        loss = model.loss(y, y_hat.to(device), weight=weight)\n","        running_loss += loss.item()\n","        # update model parameters\n","        loss.backward()\n","        optim.step()\n","        # log\n","        print(\"\\rEpoch {0}/{1}\\t- Batch {2}/{3}\\t- Average Loss {4:.02f}\\t - Time {5:.04f}s\"\n","            .format(epoch+1, epochs, i+1, len(train_dataloader), running_loss/(i+1), time() - start), end='', flush=True)\n","\n","    # add to statistic\n","    tb.Train_Loss += running_loss / len(train_dataloader)\n","\n","    # eval model\n","    model.eval()\n","    # initialize confusion matrix\n","    confusion_matrix = np.zeros((K, K))\n","    running_loss = 0\n","\n","    for x, y_hat in test_dataloader:\n","        # pass through model and compute error\n","        y = model.forward(x.to(device))\n","        running_loss += model.loss(y, y_hat.to(device), weight=weight).item()\n","        # update confusion matrix\n","        for actual, pred in zip(y_hat.flatten().cpu().numpy(), torch.argmax(y.reshape(-1, K), dim=-1).cpu().numpy()):\n","            confusion_matrix[actual, pred] += 1\n","\n","    # update board\n","    tb.Confusion += confusion_matrix\n","    tb.Test_Loss += running_loss / len(test_dataloader)\n","    # compute f-scores from confusion matrix\n","    f_scores = compute_fscores(confusion_matrix)\n","    for c, f in zip(hierarchical_class_bins[0].keys(), f_scores):\n","        tb[c] += f\n","    # save board\n","    fig = tb.create_fig([[[\"Train_Loss\", \"Test_Loss\"]], [hierarchical_class_bins[0].keys()], [[\"Confusion\"]]], figsize=(8, 11))\n","    fig.savefig(os.path.join(save_path, \"board.pdf\"), format=\"pdf\")\n","    # save model and best board if fscores improved\n","    if sum(f_scores) > best_fscore:\n","        fig.savefig(os.path.join(save_path, \"best_board.pdf\"), format=\"pdf\")\n","        model.save(save_path)\n","        best_fscore = sum(f_scores)\n","    # end epoch\n","    print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/500\t- Batch 327/327\t- Average Loss 1.23\t - Time 43.8996s\n","Epoch 2/500\t- Batch 327/327\t- Average Loss 0.96\t - Time 114.2685s\n","Epoch 3/500\t- Batch 327/327\t- Average Loss 0.85\t - Time 184.5841s\n","Epoch 4/500\t- Batch 327/327\t- Average Loss 0.79\t - Time 254.6565s\n","Epoch 5/500\t- Batch 327/327\t- Average Loss 0.76\t - Time 324.7050s\n","Epoch 6/500\t- Batch 327/327\t- Average Loss 0.73\t - Time 395.5952s\n","Epoch 7/500\t- Batch 327/327\t- Average Loss 0.70\t - Time 466.6303s\n","Epoch 8/500\t- Batch 327/327\t- Average Loss 0.65\t - Time 537.0141s\n","Epoch 9/500\t- Batch 327/327\t- Average Loss 0.62\t - Time 607.6746s\n","Epoch 10/500\t- Batch 327/327\t- Average Loss 0.59\t - Time 678.0434s\n","Epoch 11/500\t- Batch 327/327\t- Average Loss 0.57\t - Time 748.2463s\n","Epoch 12/500\t- Batch 327/327\t- Average Loss 0.55\t - Time 818.6256s\n","Epoch 13/500\t- Batch 327/327\t- Average Loss 0.53\t - Time 888.7362s\n","Epoch 14/500\t- Batch 327/327\t- Average Loss 0.52\t - Time 959.0012s\n","Epoch 15/500\t- Batch 327/327\t- Average Loss 0.51\t - Time 1029.4475s\n","Epoch 16/500\t- Batch 327/327\t- Average Loss 0.49\t - Time 1100.0768s\n","Epoch 17/500\t- Batch 327/327\t- Average Loss 0.49\t - Time 1170.1742s\n","Epoch 18/500\t- Batch 327/327\t- Average Loss 0.48\t - Time 1240.6517s\n","Epoch 19/500\t- Batch 327/327\t- Average Loss 0.47\t - Time 1311.0300s\n","Epoch 20/500\t- Batch 327/327\t- Average Loss 0.46\t - Time 1381.5038s\n","Epoch 21/500\t- Batch 327/327\t- Average Loss 0.46\t - Time 1452.0802s\n","Epoch 22/500\t- Batch 327/327\t- Average Loss 0.46\t - Time 1523.0167s\n","Epoch 23/500\t- Batch 327/327\t- Average Loss 0.45\t - Time 1594.6806s\n","Epoch 24/500\t- Batch 327/327\t- Average Loss 0.44\t - Time 1666.0382s\n","Epoch 25/500\t- Batch 327/327\t- Average Loss 0.44\t - Time 1738.0801s\n","Epoch 26/500\t- Batch 261/327\t- Average Loss 0.43\t - Time 1800.4560s"],"name":"stdout"}]}]}