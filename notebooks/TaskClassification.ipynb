{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pointnet4BerriesClassification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZPgsL_hkRIF6","colab_type":"text"},"source":["## Clone Github Repository"]},{"cell_type":"code","metadata":{"id":"z19q9N0-RNOo","colab_type":"code","outputId":"7c9c3e15-d626-49ab-96d7-befd3c878ab6","executionInfo":{"status":"ok","timestamp":1586567995128,"user_tz":-120,"elapsed":5620,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# clone repository\n","!git clone https://github.com/ndoll1998/Pointnet4Berries.git P4B"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'P4B'...\n","remote: Enumerating objects: 261, done.\u001b[K\n","remote: Counting objects: 100% (261/261), done.\u001b[K\n","remote: Compressing objects: 100% (146/146), done.\u001b[K\n","remote: Total 261 (delta 162), reused 208 (delta 111), pack-reused 0\u001b[K\n","Receiving objects: 100% (261/261), 82.57 KiB | 204.00 KiB/s, done.\n","Resolving deltas: 100% (162/162), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P7Dp_Y54RiNb","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"Y2GTB8OxRk1H","colab_type":"code","outputId":"cb751863-5170-4a69-be62-c8365393b1be","executionInfo":{"status":"ok","timestamp":1586568053290,"user_tz":-120,"elapsed":63754,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# imports\n","import sys\n","import numpy as np\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","# import model and utils\n","from P4B.Pointnet.models import Model_CLS\n","from P4B.utils.data import build_data_cls, cls_file_features\n","from P4B.utils.utils import compute_fscores\n","from P4B.utils.torchBoard import TorchBoard, ConfusionMatrix\n","from P4B.utils.augmentation import Augmenter, augment_mirror_pointcloud, augment_rotate_pointcloud\n","# import others\n","import os\n","import json\n","from time import time\n","from tqdm import tqdm\n","from random import sample\n","from collections import OrderedDict\n","# import google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h8zmuAskV259","colab_type":"text"},"source":["## Set Up"]},{"cell_type":"code","metadata":{"id":"6BLnb3KGV7Ma","colab_type":"code","colab":{}},"source":["# cude device to use\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","# class bins\n","class_bins = {'CB': ['CB'], 'D': ['D'], 'PN': ['PN'], 'R': ['R']}\n","K = len(class_bins)\n","# augmentation\n","augmentations = [] #Augmenter(augment_rotate_pointcloud, feats=cls_file_features, rot_axis='y', apply_count=5)]\n","# used features\n","features = ['x', 'y', 'z', 'r', 'g', 'b']\n","feature_dim = len(features) - 3\n","# number of points and samples\n","n_points = 40_000\n","n_samples = 4\n","# number of poinclouds per class for testing\n","n_test_pcs = 2\n","# initial checkpoint\n","encoder_init_checkpoint = None\n","classifier_init_checkpoint = None\n","# training parameters\n","epochs = 500\n","batch_size = 4\n","# optimizer parameters\n","lr = 5e-4\n","weight_decay = 1e-2\n","# path to files\n","fpath = \"drive/My Drive/P4B/data/Classification\"\n","# save path\n","save_path = \"drive/My Drive/P4B/results/classification\"\n","os.makedirs(save_path, exist_ok=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRzkJhT3WxZH","colab_type":"text"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"JnoLq8ZBW0jG","colab_type":"code","outputId":"36783b2f-667f-478d-de39-20a1ee660a11","executionInfo":{"status":"ok","timestamp":1578791494491,"user_tz":-60,"elapsed":260270,"user":{"displayName":"Niclas Doll","photoUrl":"","userId":"03509535029183763642"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pointclouds = {}\n","# open files\n","for fname in tqdm(os.listdir(fpath)):\n","    # get name of pointcloud\n","    class_name = fname.replace('.xyzrgbc', '').split('_')[0]\n","    # check for entry in pointclouds\n","    if class_name not in pointclouds:\n","        pointclouds[class_name] = []\n","    # create full path to file\n","    full_path = os.path.join(fpath, fname)\n","    # read pointcloud\n","    pointclouds[class_name].append(np.loadtxt(full_path, dtype=np.float32))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  3%|▎         | 2/74 [00:13<07:12,  6.00s/it]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"6_VEl565fytI","colab_type":"text"},"source":["## Generate Training and Testing Data"]},{"cell_type":"code","metadata":{"id":"OlAb6Phmf1h7","colab_type":"code","colab":{}},"source":["# separate pointclouds into training and testing samples\n","train_pointclouds, test_pointclouds = {}, {}\n","for class_name, pcs in pointclouds.items():\n","    # get random subset to train from\n","    train_pc_idx = sample(range(len(pcs)), len(pcs) - n_test_pcs)\n","    test_pc_idx = set(range(len(pcs))) - set(train_pc_idx)\n","    # add to dicts\n","    train_pointclouds[class_name] = [pcs[n] for n in train_pc_idx]\n","    test_pointclouds[class_name] = [pcs[n] for n in test_pc_idx]\n","# create training and testing datasets\n","train_data = TensorDataset(*build_data_cls(train_pointclouds, n_points, n_samples, class_bins, features=features, augmentations=augmentations))\n","test_data = TensorDataset(*build_data_cls(test_pointclouds, n_points, n_samples, class_bins, features=features))    # no augmentation applied to test data\n","# create training and testing dataloaders\n","train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, shuffle=False, batch_size=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ehBK372W8Sg","colab_type":"text"},"source":["## Create Model and Optimizer"]},{"cell_type":"code","metadata":{"id":"Dm0bU518XAE1","colab_type":"code","colab":{}},"source":["# create model\n","model = Model_CLS(K=K, feat_dim=feature_dim)\n","model.load_encoder(encoder_init_checkpoint)\n","model.load_classifier(classifier_init_checkpoint)\n","model.to(device)\n","# create optimizer\n","optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jD5-H-2pXDg9","colab_type":"text"},"source":["## Save Confugurations"]},{"cell_type":"code","metadata":{"id":"wyLdBW61XHXm","colab_type":"code","colab":{}},"source":["# build config\n","config = {\n","    \"task\": \"classification\",\n","    \"augmentation\": [augment.dict() for augment in augmentations],\n","    \"data\": {\n","        \"classes\": class_bins,\n","        \"features\": features,\n","        \"feature_dim\": feature_dim,\n","        \"n_points\": n_points,\n","        \"n_samples\": n_samples, \n","        \"n_test_pointclouds\": n_test_pcs,\n","        \"n_train_samples\": len(train_data),\n","        \"n_train_points\": dict(zip(class_bins.keys(), map(int, np.bincount(train_data[:][-1].flatten().numpy())))),\n","        \"n_test_samples\": len(test_data),\n","        \"n_test_points\": dict(zip(class_bins.keys(), map(int, np.bincount(test_data[:][-1].flatten().numpy())))),\n","    },\n","    \"training\": {\n","        \"epochs\": epochs,\n","        \"batch_size\": batch_size,\n","    },\n","    \"optimizer\": {\n","        \"learning_rate\": lr,\n","        \"weight_decay\": weight_decay\n","    }\n","}\n","# save to file\n","with open(os.path.join(save_path, \"config.json\"), 'w+') as f:\n","    json.dump(config, f, indent=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXFnPysXXQk9","colab_type":"text"},"source":["## Train and evaluate Model"]},{"cell_type":"code","metadata":{"id":"7PdTQu-UXVfE","colab_type":"code","colab":{}},"source":["# track losses and f-scores\n","tb = TorchBoard(\"Train_Loss\", \"Test_Loss\", *class_bins.keys())\n","tb.add_stat(ConfusionMatrix(class_bins.keys(), name=\"Confusion\", normalize=True))\n","\n","best_fscore, start = -1, time()\n","for epoch in range(epochs):\n","\n","    # train model\n","    model.train()\n","    # reset for epoch\n","    start_epoch = time()\n","    running_loss = 0\n","\n","    # train loop\n","    for i, (x, y_hat) in enumerate(train_dataloader):\n","        optim.zero_grad()\n","\n","        # pass through model\n","        y = model.forward(x.to(device))\n","        # compute error\n","        loss = model.loss(y, y_hat.to(device))\n","        running_loss += loss.item()\n","        # update model parameters\n","        loss.backward()\n","        optim.step()\n","        # log\n","        print(\"\\rEpoch {0}/{1}\\t- Batch {2}/{3}\\t- Average Loss {4:.02f}\\t - Time {5:.04f}s\"\n","            .format(epoch+1, epochs, i+1, len(train_dataloader), running_loss/(i+1), time() - start), end='', flush=True)\n","\n","    # add to statistic\n","    tb.Train_Loss += running_loss / len(train_dataloader)\n","\n","    # eval model\n","    model.eval()\n","    # initialize confusion matrix\n","    confusion_matrix = np.zeros((K, K))\n","    running_loss = 0\n","\n","    for x, y_hat in test_dataloader:\n","        # pass through model and compute error\n","        y = model.forward(x.to(device))\n","        running_loss += model.loss(y, y_hat.to(device), weight=weights).item()\n","        # update confusion matrix\n","        for actual, pred in zip(y_hat.flatten().cpu().numpy(), torch.argmax(y.reshape(-1, K), dim=-1).cpu().numpy()):\n","            confusion_matrix[actual, pred] += 1\n","\n","    # update board\n","    tb.Confusion += confusion_matrix\n","    tb.Test_Loss += running_loss / len(test_dataloader)\n","    # compute f-scores from confusion matrix\n","    f_scores = compute_fscores(confusion_matrix)\n","    for c, f in zip(class_bins.keys(), f_scores):\n","        tb[c] += f\n","    # save board\n","    fig = tb.create_fig([[[\"Train_Loss\", \"Test_Loss\"]], [class_bins.keys()], [[\"Confusion\"]]], figsize=(8, 11))\n","    fig.savefig(os.path.join(save_path, \"board.pdf\"), format=\"pdf\")\n","    # save model and best board if fscores improved\n","    if sum(f_scores) > best_fscore:\n","        fig.savefig(os.path.join(save_path, \"best_board.pdf\"), format=\"pdf\")\n","        model.save(save_path)\n","        best_fscore = sum(f_scores)\n","    # end epoch\n","    print()"],"execution_count":0,"outputs":[]}]}